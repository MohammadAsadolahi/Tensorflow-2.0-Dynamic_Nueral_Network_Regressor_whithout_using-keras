{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " Tensorflow-2.0-Dynamic_Nueral_Network_Regressor_whithout_keras.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCe6Y1OlEslr"
      },
      "source": [
        "# WRITTEN BY MOHAMMAD ASADOLAHI\n",
        "# Mohammad.E.Asadolahi@gmail.com\n",
        "# https://github.com/mohammadAsadolahi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJ_VtfzaEw0c"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ii-4qOGLE2XK"
      },
      "source": [
        "class NN:\n",
        "  def __init__(self,layers=[9,3,5,5,1],activations=None):\n",
        "    self.W=[]\n",
        "    self.B=[]\n",
        "    self.a=[]\n",
        "    if activations:\n",
        "      self.a=activations\n",
        "    else:\n",
        "      for i in  range(len(layers)-1):\n",
        "        self.a.append(tf.nn.relu)\n",
        "    for i in range(len(layers)-1):\n",
        "      self.W.append(tf.Variable(tf.random.uniform([layers[i],layers[i+1]],minval=-1,maxval=1),dtype=tf.float32))\n",
        "      self.B.append(tf.Variable(tf.random.uniform([layers[i+1]],minval=-1,maxval=1),dtype=tf.float32))\n",
        "  def predict(self,X):\n",
        "    Z=tf.Variable(np.transpose(X),dtype=tf.float32)\n",
        "    for w,b,a in zip(self.W,self.B,self.a):\n",
        "      Z=tf.add(tf.matmul(Z,w),b)\n",
        "      Z=a(Z)\n",
        "    return Z\n",
        "  def loss(self,Y_pred,Y_target):\n",
        "    return tf.reduce_mean(tf.square(Y_pred-Y_target))\n",
        "  def fit(self,X,Y,epoch,lr=0.001):\n",
        "    for i in range(epoch):\n",
        "      with tf.GradientTape() as tape:\n",
        "        cost=self.loss(self.predict(X),Y)\n",
        "        gradients=tape.gradient(cost,[self.W,self.B])\n",
        "        for dw,db,w,b in zip(gradients[0],gradients[1],self.W,self.B):\n",
        "          w.assign_sub(dw*lr)\n",
        "          b.assign_sub(db*lr)\n",
        "      if (i%100)==0:\n",
        "        print(f\"epoch:{i}  loss:\",cost.numpy())"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MARgGNLAHcn_"
      },
      "source": [
        "define and train neural net with two data  \n",
        "leads to overfit!!! (but our purpose is education of how the step of forward propagation and backpropagation are)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bm7Y1k_OFAgd",
        "outputId": "ee3c9231-edce-418e-a050-e1f834b9c8a2"
      },
      "source": [
        "neuralNet=NN(layers=[9,3,5,5,1])\n",
        "x=np.array([[1,2],[1,2],[1,2],[1,2],[1,2],[1,2],[1,2],[1,2],[1,2]])\n",
        "y=np.array([[12],[22]])\n",
        "neuralNet.fit(X=x,Y=y,epoch=3000)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:0  loss: 288.74628\n",
            "epoch:100  loss: 0.54717433\n",
            "epoch:200  loss: 0.29542917\n",
            "epoch:300  loss: 0.16419157\n",
            "epoch:400  loss: 0.092603534\n",
            "epoch:500  loss: 0.05261402\n",
            "epoch:600  loss: 0.029998153\n",
            "epoch:700  loss: 0.017128935\n",
            "epoch:800  loss: 0.009785075\n",
            "epoch:900  loss: 0.0055893185\n",
            "epoch:1000  loss: 0.003191771\n",
            "epoch:1100  loss: 0.0018220454\n",
            "epoch:1200  loss: 0.0010398753\n",
            "epoch:1300  loss: 0.0005932038\n",
            "epoch:1400  loss: 0.00033839513\n",
            "epoch:1500  loss: 0.00019297938\n",
            "epoch:1600  loss: 0.000109988025\n",
            "epoch:1700  loss: 6.272576e-05\n",
            "epoch:1800  loss: 3.5753335e-05\n",
            "epoch:1900  loss: 2.0381694e-05\n",
            "epoch:2000  loss: 1.1612721e-05\n",
            "epoch:2100  loss: 6.622968e-06\n",
            "epoch:2200  loss: 3.7705872e-06\n",
            "epoch:2300  loss: 2.1487017e-06\n",
            "epoch:2400  loss: 1.2266414e-06\n",
            "epoch:2500  loss: 6.970313e-07\n",
            "epoch:2600  loss: 3.974219e-07\n",
            "epoch:2700  loss: 2.2712811e-07\n",
            "epoch:2800  loss: 1.296903e-07\n",
            "epoch:2900  loss: 7.401468e-08\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-x0fjBzHQQ7"
      },
      "source": [
        "use neural net for predict a unseen data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "be-2cK0gGVIS",
        "outputId": "7e21ac35-57a1-459f-9b62-f6b163f5baf0"
      },
      "source": [
        "x=np.array([3,3,3,3,3,3,3,3,3])\n",
        "x=x[:,np.newaxis]\n",
        "print(neuralNet.predict(x).numpy())"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[31.999481]]\n"
          ]
        }
      ]
    }
  ]
}
